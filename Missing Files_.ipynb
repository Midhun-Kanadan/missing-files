{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47a46e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Bib File",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Bib Key",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DOI",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Publisher",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "URL",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "5a52740a-3c85-47de-bb79-34b960b5c201",
       "rows": [
        [
         "0",
         "conf-adcs-2002.bib",
         "conf-adcs-2002-quigley",
         "Liquid Mir{\\`{o}}: An Application Framework for Semantic Document Softlinking",
         null,
         null,
         "http://www.cie.ict.csiro.au/adcs2002/papers/quigley-lee.pdf"
        ],
        [
         "1",
         "conf-adcs-2002.bib",
         "conf-adcs-2002-upstill",
         "Buying bestsellers online: {A} case study in Search {\\&} Searchability",
         null,
         null,
         "http://www.cie.ict.csiro.au/adcs2002/papers/upstill-craswell-hawking.pdf"
        ],
        [
         "2",
         "conf-adcs-2002.bib",
         "conf-adcs-2002-davis",
         "Workflow Based Just-in-time Training",
         null,
         null,
         "http://www.cie.ict.csiro.au/adcs2002/papers/davis-et-al.pdf"
        ],
        [
         "3",
         "conf-adcs-2002.bib",
         "conf-adcs-2002-eklund",
         "Visual Displays for Browsing {RDF} Documents",
         null,
         null,
         "http://www.cie.ict.csiro.au/adcs2002/papers/eklund.pdf"
        ],
        [
         "4",
         "conf-adcs-2002.bib",
         "conf-adcs-2002-schwitter",
         "How to Write a Document in Controlled Natural Language",
         null,
         null,
         "http://www.cie.ict.csiro.au/adcs2002/papers/schwitter-ljungberg.pdf"
        ],
        [
         "5",
         "conf-adcs-2002.bib",
         "conf-adcs-2002-boparai",
         "Supporting user task based conversations via e-mail",
         null,
         null,
         "http://www.cie.ict.csiro.au/adcs2002/papers/boparai-kay.pdf"
        ],
        [
         "6",
         "conf-adcs-2002.bib",
         "conf-adcs-2002-heyer",
         "Tibianna: {A} Learning-Based Search Engine with Query Refinement",
         null,
         null,
         "http://www.cie.ict.csiro.au/adcs2002/papers/heyer-diederich.pdf"
        ],
        [
         "7",
         "conf-adcs-2002.bib",
         "conf-adcs-2002-calvo",
         "Automatic Categorization of Announcements on the {ASX}",
         null,
         null,
         "http://www.cie.ict.csiro.au/adcs2002/papers/calvo-williams.pdf"
        ],
        [
         "8",
         "conf-adcs-2002.bib",
         "conf-adcs-2002-heyer-2",
         "MyNewsWave: User-centered Web search and news delivery",
         null,
         null,
         "http://www.cie.ict.csiro.au/adcs2002/papers/heyer-et-al.pdf"
        ],
        [
         "9",
         "conf-adcs-2002.bib",
         "conf-adcs-2002-barta",
         "Managing Literature References with Topic Maps",
         null,
         null,
         "http://www.cie.ict.csiro.au/adcs2002/papers/barta-kelly.pdf"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bib File</th>\n",
       "      <th>Bib Key</th>\n",
       "      <th>Title</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conf-adcs-2002.bib</td>\n",
       "      <td>conf-adcs-2002-quigley</td>\n",
       "      <td>Liquid Mir{\\`{o}}: An Application Framework fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cie.ict.csiro.au/adcs2002/papers/qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conf-adcs-2002.bib</td>\n",
       "      <td>conf-adcs-2002-upstill</td>\n",
       "      <td>Buying bestsellers online: {A} case study in S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cie.ict.csiro.au/adcs2002/papers/up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conf-adcs-2002.bib</td>\n",
       "      <td>conf-adcs-2002-davis</td>\n",
       "      <td>Workflow Based Just-in-time Training</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cie.ict.csiro.au/adcs2002/papers/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conf-adcs-2002.bib</td>\n",
       "      <td>conf-adcs-2002-eklund</td>\n",
       "      <td>Visual Displays for Browsing {RDF} Documents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cie.ict.csiro.au/adcs2002/papers/ek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conf-adcs-2002.bib</td>\n",
       "      <td>conf-adcs-2002-schwitter</td>\n",
       "      <td>How to Write a Document in Controlled Natural ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cie.ict.csiro.au/adcs2002/papers/sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>conf-adcs-2002.bib</td>\n",
       "      <td>conf-adcs-2002-boparai</td>\n",
       "      <td>Supporting user task based conversations via e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cie.ict.csiro.au/adcs2002/papers/bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>conf-adcs-2002.bib</td>\n",
       "      <td>conf-adcs-2002-heyer</td>\n",
       "      <td>Tibianna: {A} Learning-Based Search Engine wit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cie.ict.csiro.au/adcs2002/papers/he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>conf-adcs-2002.bib</td>\n",
       "      <td>conf-adcs-2002-calvo</td>\n",
       "      <td>Automatic Categorization of Announcements on t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cie.ict.csiro.au/adcs2002/papers/ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conf-adcs-2002.bib</td>\n",
       "      <td>conf-adcs-2002-heyer-2</td>\n",
       "      <td>MyNewsWave: User-centered Web search and news ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cie.ict.csiro.au/adcs2002/papers/he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>conf-adcs-2002.bib</td>\n",
       "      <td>conf-adcs-2002-barta</td>\n",
       "      <td>Managing Literature References with Topic Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cie.ict.csiro.au/adcs2002/papers/ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Bib File                   Bib Key  \\\n",
       "0  conf-adcs-2002.bib    conf-adcs-2002-quigley   \n",
       "1  conf-adcs-2002.bib    conf-adcs-2002-upstill   \n",
       "2  conf-adcs-2002.bib      conf-adcs-2002-davis   \n",
       "3  conf-adcs-2002.bib     conf-adcs-2002-eklund   \n",
       "4  conf-adcs-2002.bib  conf-adcs-2002-schwitter   \n",
       "5  conf-adcs-2002.bib    conf-adcs-2002-boparai   \n",
       "6  conf-adcs-2002.bib      conf-adcs-2002-heyer   \n",
       "7  conf-adcs-2002.bib      conf-adcs-2002-calvo   \n",
       "8  conf-adcs-2002.bib    conf-adcs-2002-heyer-2   \n",
       "9  conf-adcs-2002.bib      conf-adcs-2002-barta   \n",
       "\n",
       "                                               Title  DOI Publisher  \\\n",
       "0  Liquid Mir{\\`{o}}: An Application Framework fo...  NaN       NaN   \n",
       "1  Buying bestsellers online: {A} case study in S...  NaN       NaN   \n",
       "2               Workflow Based Just-in-time Training  NaN       NaN   \n",
       "3       Visual Displays for Browsing {RDF} Documents  NaN       NaN   \n",
       "4  How to Write a Document in Controlled Natural ...  NaN       NaN   \n",
       "5  Supporting user task based conversations via e...  NaN       NaN   \n",
       "6  Tibianna: {A} Learning-Based Search Engine wit...  NaN       NaN   \n",
       "7  Automatic Categorization of Announcements on t...  NaN       NaN   \n",
       "8  MyNewsWave: User-centered Web search and news ...  NaN       NaN   \n",
       "9     Managing Literature References with Topic Maps  NaN       NaN   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://www.cie.ict.csiro.au/adcs2002/papers/qu...  \n",
       "1  http://www.cie.ict.csiro.au/adcs2002/papers/up...  \n",
       "2  http://www.cie.ict.csiro.au/adcs2002/papers/da...  \n",
       "3  http://www.cie.ict.csiro.au/adcs2002/papers/ek...  \n",
       "4  http://www.cie.ict.csiro.au/adcs2002/papers/sc...  \n",
       "5  http://www.cie.ict.csiro.au/adcs2002/papers/bo...  \n",
       "6  http://www.cie.ict.csiro.au/adcs2002/papers/he...  \n",
       "7  http://www.cie.ict.csiro.au/adcs2002/papers/ca...  \n",
       "8  http://www.cie.ict.csiro.au/adcs2002/papers/he...  \n",
       "9  http://www.cie.ict.csiro.au/adcs2002/papers/ba...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# Load updated CSV file\n",
    "df = pd.read_csv(\"missing_pdfs.csv\")\n",
    "\n",
    "# Filter entries with a valid URL\n",
    "valid_urls_df = df[df[\"URL\"].notnull()]\n",
    "valid_urls_df = valid_urls_df[:10]\n",
    "valid_urls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957da5b9",
   "metadata": {},
   "source": [
    "### Combine the processed batches into a single CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc788db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All batches merged into 'combined_updated_missing_pdfs.csv'\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Combine all enriched CSVs\n",
    "files = sorted(glob.glob(\"Processed-files/batch_*.csv\"))\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the final enriched dataset\n",
    "final_df.to_csv(\"combined_updated_missing_pdfs.csv\", index=False)\n",
    "print(\"✅ All batches merged into 'combined_updated_missing_pdfs.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81373b7",
   "metadata": {},
   "source": [
    "## download the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "csv_path = \"missing_pdfs.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "subset_df = df[df[\"URL\"].str.startswith(\"http\", na=False)].head(5)\n",
    "\n",
    "\n",
    "output_folder = \"downloaded_pdfs_subset\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Setup User-Agent rotation\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\"\n",
    "]\n",
    "\n",
    "# === Step 5: Download the PDFs ===\n",
    "for index, row in subset_df.iterrows():\n",
    "    url = row[\"URL\"]\n",
    "    bib_key = row[\"Bib Key\"]\n",
    "    filename = os.path.join(output_folder, f\"{bib_key}.pdf\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Attempting: {url}\")\n",
    "        headers = {\"User-Agent\": random.choice(user_agents)}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        if response.status_code == 200 and \"application/pdf\" in content_type:\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"✅ Downloaded: {filename}\")\n",
    "        else:\n",
    "            print(f\"❌ Skipped (not a PDF or bad response): {url}\")\n",
    "\n",
    "        time.sleep(random.uniform(3, 7))  # Human-like wait\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error downloading {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c945bb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to: acm_filtered_pdfs.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"missing_pdfs.csv\")\n",
    "\n",
    "# Define target publishers\n",
    "target_publishers = [\"{ACM}\", \"{ACM} / {IW3C2}\", \"{ACM} Press\"]\n",
    "\n",
    "# Filter the rows\n",
    "filtered_df = df[df['Publisher'].isin(target_publishers)]\n",
    "\n",
    "# Save the filtered rows to a new CSV file\n",
    "output_path = \"acm_filtered_pdfs.csv\"\n",
    "filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bc044",
   "metadata": {},
   "source": [
    "## === Final Cleanup ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "import browser_cookie3\n",
    "from urllib.parse import quote\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "CSV_PATH = \"missing_pdfs.csv\"\n",
    "OUTPUT_FOLDER = \"acm_pdf_test_subset\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# === Load cookies from your browser (must be logged in to ACM) ===\n",
    "cookies = browser_cookie3.load(domain_name='dl.acm.org')\n",
    "\n",
    "# === Load CSV and filter rows with DOI-based URLs ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "doi_df = df[df[\"URL\"].str.contains(\"10\\\\.\", na=False)].copy()\n",
    "subset_df = doi_df.head(5)  # <-- Only first 5 for testing\n",
    "\n",
    "# === Convert DOI URL to direct PDF URL ===\n",
    "def doi_to_pdf_url(url):\n",
    "    import re\n",
    "    match = re.search(r\"(10\\.\\d{4,9}/[^\\s]+)\", url)\n",
    "    if match:\n",
    "        doi = match.group(1)\n",
    "        return f\"https://dl.acm.org/action/showPdf?doi={quote(doi)}\"\n",
    "    return None\n",
    "\n",
    "# === User-Agent list ===\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\"\n",
    "]\n",
    "\n",
    "# === Download loop ===\n",
    "failed = []\n",
    "\n",
    "for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), desc=\"Testing ACM PDF Download\"):\n",
    "    bib_key = row[\"Bib Key\"]\n",
    "    original_url = row[\"URL\"]\n",
    "    pdf_url = doi_to_pdf_url(original_url)\n",
    "\n",
    "    if not pdf_url:\n",
    "        print(f\"❌ Skipping invalid DOI: {original_url}\")\n",
    "        failed.append((bib_key, original_url, \"Invalid DOI\"))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "        response = requests.get(pdf_url, headers=headers, cookies=cookies, timeout=20)\n",
    "\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        if response.status_code == 200 and \"application/pdf\" in content_type:\n",
    "            filename = os.path.join(OUTPUT_FOLDER, f\"{bib_key}.pdf\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"✅ Downloaded: {filename}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Failed to download: {pdf_url} → {content_type}\")\n",
    "            failed.append((bib_key, pdf_url, f\"{response.status_code}, {content_type}\"))\n",
    "\n",
    "        time.sleep(random.uniform(3, 6))  # Human-like delay\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for {pdf_url}: {e}\")\n",
    "        failed.append((bib_key, pdf_url, str(e)))\n",
    "\n",
    "# === Save failures (optional) ===\n",
    "if failed:\n",
    "    pd.DataFrame(failed, columns=[\"Bib Key\", \"URL\", \"Reason\"]).to_csv(\"acm_failed_test_subset.csv\", index=False)\n",
    "    print(f\"\\n⚠️ Some downloads failed. Check acm_failed_test_subset.csv\")\n",
    "else:\n",
    "    print(\"\\n🎉 All test downloads successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ab01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "import browser_cookie3\n",
    "from urllib.parse import quote\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "CSV_PATH = \"acm_filtered_pdfs.csv\"\n",
    "OUTPUT_FOLDER = \"acm_pdf_test_subset\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# === Load cookies from your browser (must be logged in to ACM) ===\n",
    "cookies = browser_cookie3.load(domain_name='dl.acm.org')\n",
    "\n",
    "# === Load CSV and filter rows with DOI-based URLs ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "doi_df = df[df[\"URL\"].str.contains(\"10\\\\.\", na=False)].copy()\n",
    "subset_df = doi_df.head(5)  # <-- Only first 5 for testing\n",
    "\n",
    "# === Convert DOI URL to direct PDF URL ===\n",
    "def doi_to_pdf_url(url):\n",
    "    import re\n",
    "    match = re.search(r\"(10\\.\\d{4,9}/[^\\s]+)\", url)\n",
    "    if match:\n",
    "        doi = match.group(1)\n",
    "        return f\"https://dl.acm.org/doi/pdf/{quote(doi)}\"\n",
    "    return None\n",
    "\n",
    "# === User-Agent list ===\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\"\n",
    "]\n",
    "\n",
    "# === Download loop ===\n",
    "failed = []\n",
    "\n",
    "for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), desc=\"Testing ACM PDF Download\"):\n",
    "    bib_key = row[\"Bib Key\"]\n",
    "    original_url = row[\"URL\"]\n",
    "    pdf_url = doi_to_pdf_url(original_url)\n",
    "\n",
    "    if not pdf_url:\n",
    "        print(f\"❌ Skipping invalid DOI: {original_url}\")\n",
    "        failed.append((bib_key, original_url, \"Invalid DOI\"))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "        response = requests.get(pdf_url, headers=headers, cookies=cookies, timeout=20)\n",
    "\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        if response.status_code == 200 and \"application/pdf\" in content_type:\n",
    "            filename = os.path.join(OUTPUT_FOLDER, f\"{bib_key}.pdf\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"✅ Downloaded: {filename}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Failed to download: {pdf_url} → {content_type}\")\n",
    "            failed.append((bib_key, pdf_url, f\"{response.status_code}, {content_type}\"))\n",
    "\n",
    "        time.sleep(random.uniform(3, 6))  # Human-like delay\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for {pdf_url}: {e}\")\n",
    "        failed.append((bib_key, pdf_url, str(e)))\n",
    "\n",
    "# === Save failures (optional) ===\n",
    "if failed:\n",
    "    pd.DataFrame(failed, columns=[\"Bib Key\", \"URL\", \"Reason\"]).to_csv(\"acm_failed_test_subset.csv\", index=False)\n",
    "    print(f\"\\n⚠️ Some downloads failed. Check acm_failed_test_subset.csv\")\n",
    "else:\n",
    "    print(\"\\n🎉 All test downloads successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e575ace-a916-4e5f-a9df-8c95f7e1dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Configuration ===\n",
    "CSV_PATH = \"missing_pdfs.csv\"\n",
    "OUTPUT_FOLDER = \"acm_pdf_test_subset\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# === Load CSV and filter for DOI-containing URLs ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "doi_df = df[df[\"URL\"].str.contains(\"10\\\\.\", na=False)].copy()\n",
    "subset_df = doi_df.head(2)  # <-- Test on first 5\n",
    "\n",
    "def doi_to_pdf_url(url):\n",
    "    import re\n",
    "    match = re.search(r\"(10\\.\\d{4,9}/[^\\s]+)\", url)\n",
    "    if match:\n",
    "        doi = match.group(1)\n",
    "        return f\"https://dl.acm.org/action/showPdf?doi={quote(doi)}\"\n",
    "    return None\n",
    "\n",
    "# === Headers to mimic browser ===\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\"\n",
    "]\n",
    "\n",
    "# === Download loop ===\n",
    "failed = []\n",
    "\n",
    "for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), desc=\"Downloading PDFs\"):\n",
    "    bib_key = row[\"Bib Key\"]\n",
    "    original_url = row[\"URL\"]\n",
    "    pdf_url = doi_to_pdf_url(original_url)\n",
    "\n",
    "    if not pdf_url:\n",
    "        print(f\"❌ Invalid DOI in URL: {original_url}\")\n",
    "        failed.append((bib_key, original_url, \"Invalid DOI\"))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "        response = requests.get(pdf_url, headers=headers, timeout=20)\n",
    "\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        if response.status_code == 200 and \"application/pdf\" in content_type:\n",
    "            filename = os.path.join(OUTPUT_FOLDER, f\"{bib_key}.pdf\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"✅ Downloaded: {filename}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Not a PDF or no access: {pdf_url}\")\n",
    "            failed.append((bib_key, pdf_url, f\"{response.status_code}, {content_type}\"))\n",
    "\n",
    "        time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        failed.append((bib_key, pdf_url, str(e)))\n",
    "\n",
    "# === Save failed attempts ===\n",
    "if failed:\n",
    "    pd.DataFrame(failed, columns=[\"Bib Key\", \"URL\", \"Reason\"]).to_csv(\"acm_failed_test_subset.csv\", index=False)\n",
    "    print(\"\\n⚠️ Some downloads failed. Check 'acm_failed_test_subset.csv'\")\n",
    "else:\n",
    "    print(\"\\n🎉 All test downloads successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a0ce8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading via /doi/pdf: 100%|██████████| 5/5 [00:20<00:00,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ Some downloads failed. Check 'acm_doi_pdf_fails.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Config ===\n",
    "CSV_PATH = \"missing_pdfs.csv\"\n",
    "OUTPUT_FOLDER = \"acm_pdf_doi_fallback_test\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "doi_df = df[df[\"URL\"].str.contains(\"10\\\\.\", na=False)].copy()\n",
    "subset_df = doi_df.head(5)  # First 5 for test\n",
    "\n",
    "# === Convert to /doi/pdf/<DOI> format ===\n",
    "def doi_to_pdf_url(url):\n",
    "    import re\n",
    "    match = re.search(r\"(10\\.\\d{4,9}/[^\\s]+)\", url)\n",
    "    if match:\n",
    "        doi = match.group(1)\n",
    "        return f\"https://dl.acm.org/doi/pdf/{quote(doi)}\"\n",
    "    return None\n",
    "\n",
    "# === Headers to mimic real browser ===\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Accept\": \"application/pdf\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://dl.acm.org/\",\n",
    "}\n",
    "\n",
    "# === Download loop ===\n",
    "failed = []\n",
    "\n",
    "for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), desc=\"Downloading via /doi/pdf\"):\n",
    "    bib_key = row[\"Bib Key\"]\n",
    "    original_url = row[\"URL\"]\n",
    "    pdf_url = doi_to_pdf_url(original_url)\n",
    "\n",
    "    if not pdf_url:\n",
    "        failed.append((bib_key, original_url, \"Invalid DOI\"))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = requests.get(pdf_url, headers=HEADERS, timeout=20, allow_redirects=True)\n",
    "\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        if response.status_code == 200 and \"application/pdf\" in content_type:\n",
    "            filename = os.path.join(OUTPUT_FOLDER, f\"{bib_key}.pdf\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"✅ Downloaded: {filename}\")\n",
    "        else:\n",
    "            failed.append((bib_key, pdf_url, f\"{response.status_code}, {content_type}\"))\n",
    "\n",
    "        time.sleep(random.uniform(3, 6))  # Human-like delay\n",
    "\n",
    "    except Exception as e:\n",
    "        failed.append((bib_key, pdf_url, str(e)))\n",
    "\n",
    "# === Save failures (optional) ===\n",
    "if failed:\n",
    "    pd.DataFrame(failed, columns=[\"Bib Key\", \"URL\", \"Reason\"]).to_csv(\"acm_doi_pdf_fails.csv\", index=False)\n",
    "    print(\"\\n⚠️ Some downloads failed. Check 'acm_doi_pdf_fails.csv'\")\n",
    "else:\n",
    "    print(\"\\n🎉 All test downloads successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Create download directory\n",
    "download_dir = os.path.join(os.getcwd(), \"acm_pdfs\")\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Configure undetected Chrome\n",
    "options = uc.ChromeOptions()\n",
    "prefs = {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"plugins.always_open_pdf_externally\": True,\n",
    "}\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "# Launch Chrome (undetected)\n",
    "driver = uc.Chrome(options=options, headless=False)\n",
    "\n",
    "# List of DOIs\n",
    "doi_list = [\n",
    "    \"10.1145/2537734.2537741\",\n",
    "]\n",
    "\n",
    "for doi in doi_list:\n",
    "    url = f\"https://dl.acm.org/doi/{doi}\"\n",
    "    print(f\"Opening: {url}\")\n",
    "    driver.get(url)\n",
    "\n",
    "    # Give time for manual CAPTCHA solving or auto-bypass\n",
    "    print(\"🕵️‍♂️ Solve CAPTCHA if asked, or wait for bypass...\")b\n",
    "    time.sleep(20)\n",
    "\n",
    "    try:\n",
    "        # Try to click on the PDF button\n",
    "        pdf_button = driver.find_element(By.XPATH, \"//a[contains(text(), 'PDF')]\")\n",
    "        pdf_button.click()\n",
    "        print(\"✅ PDF download started.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Couldn't find the PDF button: {e}\")\n",
    "\n",
    "    time.sleep(random.uniform(10, 20))\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a58f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Configuration ===\n",
    "CSV_PATH = \"acm_filtered_pdfs.csv\"\n",
    "OUTPUT_FOLDER = \"acm_pdf_test_subset\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# === Load CSV and filter for DOI-containing URLs ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "doi_df = df[df[\"URL\"].str.contains(\"10\\\\.\", na=False)].copy()\n",
    "subset_df = doi_df.head(5)  # <-- Test on first 5\n",
    "\n",
    "# === Convert to /action/showPdf URL ===\n",
    "def doi_to_pdf_url(url):\n",
    "    import re\n",
    "    match = re.search(r\"(10\\.\\d{4,9}/[^\\s]+)\", url)\n",
    "    if match:\n",
    "        doi = match.group(1)\n",
    "        # return f\"https://dl.acm.org/action/showPdf?doi={quote(doi)}\"\n",
    "        return f\"https://dl.acm.org/doi/pdf/{quote(doi)}\"\n",
    "    return None\n",
    "\n",
    "# === Headers to mimic browser ===\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)\"\n",
    "]\n",
    "\n",
    "# === Download loop ===\n",
    "failed = []\n",
    "\n",
    "for _, row in tqdm(subset_df.iterrows(), total=len(subset_df), desc=\"Downloading PDFs\"):\n",
    "    bib_key = row[\"Bib Key\"]\n",
    "    original_url = row[\"URL\"]\n",
    "    pdf_url = doi_to_pdf_url(original_url)\n",
    "\n",
    "    if not pdf_url:\n",
    "        print(f\"❌ Invalid DOI in URL: {original_url}\")\n",
    "        failed.append((bib_key, original_url, \"Invalid DOI\"))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "        response = requests.get(pdf_url, headers=headers, timeout=20)\n",
    "\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        if response.status_code == 200 and \"application/pdf\" in content_type:\n",
    "            filename = os.path.join(OUTPUT_FOLDER, f\"{bib_key}.pdf\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"✅ Downloaded: {filename}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Not a PDF or no access: {pdf_url}\")\n",
    "            failed.append((bib_key, pdf_url, f\"{response.status_code}, {content_type}\"))\n",
    "\n",
    "        time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        failed.append((bib_key, pdf_url, str(e)))\n",
    "\n",
    "# === Save failed attempts ===\n",
    "if failed:\n",
    "    pd.DataFrame(failed, columns=[\"Bib Key\", \"URL\", \"Reason\"]).to_csv(\"acm_failed_test_subset.csv\", index=False)\n",
    "    print(\"\\n⚠️ Some downloads failed. Check 'acm_failed_test_subset.csv'\")\n",
    "else:\n",
    "    print(\"\\n🎉 All test downloads successful!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
